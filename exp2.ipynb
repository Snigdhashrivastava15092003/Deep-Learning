{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load data from CSV file\n",
        "data = pd.read_csv(\"DL2.csv\")\n",
        "\n",
        "# Assuming the last column is the label, and rest are features\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def create_model(neurons_1, neurons_2, optimizer):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neurons_1, input_dim=X_train.shape[1], activation='relu'))\n",
        "    model.add(Dense(neurons_2, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define different configurations\n",
        "configurations = [\n",
        "    (64, 64, 'adam'),\n",
        "    (128, 128, 'adam'),\n",
        "    (64, 128, 'adam'),\n",
        "    (128, 64, 'adam'),\n",
        "]\n",
        "\n",
        "# Store results in a dictionary\n",
        "results = {}\n",
        "\n",
        "# Train models and store results\n",
        "for i, (neurons_1, neurons_2, optimizer) in enumerate(configurations):\n",
        "    model = create_model(neurons_1, neurons_2, optimizer)\n",
        "    history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
        "    results[f'Model_{i+1}'] = {\n",
        "        'optimizer': optimizer,\n",
        "        'train_accuracy': history.history['accuracy'][-1],\n",
        "        'val_accuracy': history.history['val_accuracy'][-1],\n",
        "        'train_loss': history.history['loss'][-1],\n",
        "        'val_loss': history.history['val_loss'][-1],\n",
        "    }\n",
        "\n",
        "# Display results in a table\n",
        "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
        "print(results_df)\n",
        "\n",
        "# Predict one value using the first model\n",
        "sample_input = X_val[0].reshape(1, -1)\n",
        "predicted_value = model.predict(sample_input)\n",
        "print(f\"Predicted Value for Sample Input: {predicted_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyArjd8Ogqwg",
        "outputId": "549edf51-3a08-4fee-d583-5d806ce983a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        optimizer  train_accuracy  val_accuracy  train_loss    val_loss\n",
            "Model_1      adam             0.0           0.0 -363.800171 -335.750580\n",
            "Model_2      adam             0.0           0.0 -404.151123 -349.862701\n",
            "Model_3      adam             0.0           0.0 -302.087708 -248.044632\n",
            "Model_4      adam             0.0           0.0 -300.010834 -213.132080\n",
            "1/1 [==============================] - 0s 88ms/step\n",
            "Predicted Value for Sample Input: [[0.9999922]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## exp5"
      ],
      "metadata": {
        "id": "JeC3LpPMdx4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.optimizers import legacy\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('diabetes2.0.csv')\n",
        "\n",
        "# Split features and target\n",
        "X = data.drop('BloodPressure', axis=1)\n",
        "y = data['BloodPressure']\n",
        "\n",
        "# Splitting into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Define the architecture\n",
        "architectures = [\n",
        "    (64, 64),\n",
        "    (128, 128),\n",
        "    (64, 128),\n",
        "    (128, 64)\n",
        "]\n",
        "\n",
        "# Define optimizer options\n",
        "optimizers = [\n",
        "    ('Adam', legacy.Adam()),\n",
        "    ('SGD', legacy.SGD()),\n",
        "    ('RMSprop', legacy.RMSprop())\n",
        "]\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# Iterate over architectures and optimizers\n",
        "for arch in architectures:\n",
        "    for opt_name, opt in optimizers:\n",
        "        model = Sequential()\n",
        "        model.add(Dense(arch[0], activation='relu', input_shape=(X_train.shape[1],)))\n",
        "        model.add(Dense(arch[1], activation='relu'))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        history = model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=0)\n",
        "\n",
        "        key = f\"{arch}-{opt_name}\"\n",
        "        results[key] = {\n",
        "            'train_accuracy': history.history['accuracy'],\n",
        "            'val_accuracy': history.history['val_accuracy'],\n",
        "            'train_loss': history.history['loss'],\n",
        "            'val_loss': history.history['val_loss']\n",
        "        }\n",
        "\n",
        "# Print results\n",
        "print(\"\\t\\tTrain Accuracy\\tVal Accuracy\\tTrain Loss\\tVal Loss\")\n",
        "for key, value in results.items():\n",
        "    print(key)\n",
        "    epochs = len(value['train_accuracy'])\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}:\\t{value['train_accuracy'][epoch]:.4f}\\t\\t{value['val_accuracy'][epoch]:.4f}\\t\\t{value['train_loss'][epoch]:.4f}\\t\\t{value['val_loss'][epoch]:.4f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "334koasbfVYu",
        "outputId": "ae942537-cd14-478c-a572-150351f52de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\tTrain Accuracy\tVal Accuracy\tTrain Loss\tVal Loss\n",
            "(64, 64)-Adam\n",
            "Epoch 1:\t0.0016\t\t0.0000\t\t-62.0116\t\t-129.8361\n",
            "Epoch 2:\t0.0000\t\t0.0000\t\t-211.1181\t\t-331.3201\n",
            "Epoch 3:\t0.0000\t\t0.0000\t\t-480.8654\t\t-694.1302\n",
            "Epoch 4:\t0.0000\t\t0.0000\t\t-950.9773\t\t-1301.4757\n",
            "Epoch 5:\t0.0000\t\t0.0000\t\t-1707.2314\t\t-2260.8518\n",
            "Epoch 6:\t0.0000\t\t0.0000\t\t-2884.0215\t\t-3693.3418\n",
            "Epoch 7:\t0.0000\t\t0.0000\t\t-4584.5737\t\t-5746.3257\n",
            "Epoch 8:\t0.0000\t\t0.0000\t\t-6992.3022\t\t-8568.0010\n",
            "Epoch 9:\t0.0000\t\t0.0000\t\t-10217.8662\t\t-12266.5625\n",
            "Epoch 10:\t0.0000\t\t0.0000\t\t-14360.6855\t\t-17060.4766\n",
            "\n",
            "(64, 64)-SGD\n",
            "Epoch 1:\t0.0163\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 2:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 3:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 4:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 5:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 6:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 7:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 8:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 9:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 10:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "\n",
            "(64, 64)-RMSprop\n",
            "Epoch 1:\t0.0016\t\t0.0000\t\t-102.4663\t\t-210.7914\n",
            "Epoch 2:\t0.0000\t\t0.0000\t\t-329.8833\t\t-475.2292\n",
            "Epoch 3:\t0.0000\t\t0.0000\t\t-634.4576\t\t-824.8916\n",
            "Epoch 4:\t0.0000\t\t0.0000\t\t-1039.7115\t\t-1295.0321\n",
            "Epoch 5:\t0.0000\t\t0.0000\t\t-1564.1714\t\t-1870.0868\n",
            "Epoch 6:\t0.0000\t\t0.0000\t\t-2202.5161\t\t-2573.9968\n",
            "Epoch 7:\t0.0000\t\t0.0000\t\t-2987.9539\t\t-3437.7097\n",
            "Epoch 8:\t0.0000\t\t0.0000\t\t-3938.0227\t\t-4478.8936\n",
            "Epoch 9:\t0.0000\t\t0.0000\t\t-5061.3076\t\t-5690.3667\n",
            "Epoch 10:\t0.0000\t\t0.0000\t\t-6364.3413\t\t-7064.0859\n",
            "\n",
            "(128, 128)-Adam\n",
            "Epoch 1:\t0.0000\t\t0.0000\t\t-362.0904\t\t-1093.7468\n",
            "Epoch 2:\t0.0000\t\t0.0000\t\t-2463.4011\t\t-4649.0107\n",
            "Epoch 3:\t0.0000\t\t0.0000\t\t-7696.2866\t\t-12034.6475\n",
            "Epoch 4:\t0.0000\t\t0.0000\t\t-17212.7598\t\t-24157.0645\n",
            "Epoch 5:\t0.0000\t\t0.0000\t\t-31657.9551\t\t-41685.5000\n",
            "Epoch 6:\t0.0000\t\t0.0000\t\t-52066.0938\t\t-65161.9609\n",
            "Epoch 7:\t0.0000\t\t0.0000\t\t-78502.4922\t\t-95507.6641\n",
            "Epoch 8:\t0.0000\t\t0.0000\t\t-111689.9531\t\t-132585.3281\n",
            "Epoch 9:\t0.0000\t\t0.0000\t\t-152238.2344\t\t-176174.0156\n",
            "Epoch 10:\t0.0000\t\t0.0000\t\t-199739.0469\t\t-228355.4219\n",
            "\n",
            "(128, 128)-SGD\n",
            "Epoch 1:\t0.0163\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 2:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 3:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 4:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 5:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 6:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 7:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 8:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 9:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 10:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "\n",
            "(128, 128)-RMSprop\n",
            "Epoch 1:\t0.0033\t\t0.0000\t\t-184.3902\t\t-432.3493\n",
            "Epoch 2:\t0.0000\t\t0.0000\t\t-678.7879\t\t-1046.5125\n",
            "Epoch 3:\t0.0000\t\t0.0000\t\t-1428.3586\t\t-1980.7104\n",
            "Epoch 4:\t0.0000\t\t0.0000\t\t-2508.1809\t\t-3274.4729\n",
            "Epoch 5:\t0.0000\t\t0.0000\t\t-3966.9158\t\t-4976.4927\n",
            "Epoch 6:\t0.0000\t\t0.0000\t\t-5853.1860\t\t-7179.4277\n",
            "Epoch 7:\t0.0000\t\t0.0000\t\t-8207.7646\t\t-9773.5791\n",
            "Epoch 8:\t0.0000\t\t0.0000\t\t-11047.2871\t\t-12931.8135\n",
            "Epoch 9:\t0.0000\t\t0.0000\t\t-14513.3857\t\t-16879.3184\n",
            "Epoch 10:\t0.0000\t\t0.0000\t\t-18640.4941\t\t-21457.2129\n",
            "\n",
            "(64, 128)-Adam\n",
            "Epoch 1:\t0.0049\t\t0.0000\t\t-223.9219\t\t-748.3671\n",
            "Epoch 2:\t0.0000\t\t0.0000\t\t-1780.4996\t\t-3540.7683\n",
            "Epoch 3:\t0.0000\t\t0.0000\t\t-5904.5439\t\t-9624.6650\n",
            "Epoch 4:\t0.0000\t\t0.0000\t\t-13665.2129\t\t-19786.4746\n",
            "Epoch 5:\t0.0000\t\t0.0000\t\t-25911.2520\t\t-34669.4375\n",
            "Epoch 6:\t0.0000\t\t0.0000\t\t-42762.6133\t\t-54800.5078\n",
            "Epoch 7:\t0.0000\t\t0.0000\t\t-64915.4648\t\t-80184.3906\n",
            "Epoch 8:\t0.0000\t\t0.0000\t\t-92214.6484\t\t-110902.0234\n",
            "Epoch 9:\t0.0000\t\t0.0000\t\t-124528.4141\t\t-147164.3906\n",
            "Epoch 10:\t0.0000\t\t0.0000\t\t-162662.7656\t\t-189315.6250\n",
            "\n",
            "(64, 128)-SGD\n",
            "Epoch 1:\t0.0195\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 2:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 3:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 4:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 5:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 6:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 7:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 8:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 9:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 10:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "\n",
            "(64, 128)-RMSprop\n",
            "Epoch 1:\t0.0016\t\t0.0000\t\t-112.1848\t\t-267.4419\n",
            "Epoch 2:\t0.0000\t\t0.0000\t\t-433.3640\t\t-667.1804\n",
            "Epoch 3:\t0.0000\t\t0.0000\t\t-901.4968\t\t-1226.7926\n",
            "Epoch 4:\t0.0000\t\t0.0000\t\t-1559.9525\t\t-1987.3851\n",
            "Epoch 5:\t0.0000\t\t0.0000\t\t-2419.5896\t\t-2984.3101\n",
            "Epoch 6:\t0.0000\t\t0.0000\t\t-3534.8311\t\t-4267.1299\n",
            "Epoch 7:\t0.0000\t\t0.0000\t\t-4931.1299\t\t-5807.3599\n",
            "Epoch 8:\t0.0000\t\t0.0000\t\t-6609.0898\t\t-7692.6274\n",
            "Epoch 9:\t0.0000\t\t0.0000\t\t-8636.3623\t\t-9904.5537\n",
            "Epoch 10:\t0.0000\t\t0.0000\t\t-11007.0498\t\t-12553.5078\n",
            "\n",
            "(128, 64)-Adam\n",
            "Epoch 1:\t0.0000\t\t0.0000\t\t-421.1556\t\t-1449.9282\n",
            "Epoch 2:\t0.0000\t\t0.0000\t\t-3580.4988\t\t-7216.9941\n",
            "Epoch 3:\t0.0000\t\t0.0000\t\t-12065.3857\t\t-19695.1152\n",
            "Epoch 4:\t0.0000\t\t0.0000\t\t-27767.5840\t\t-40137.9414\n",
            "Epoch 5:\t0.0000\t\t0.0000\t\t-51637.4336\t\t-69091.2344\n",
            "Epoch 6:\t0.0000\t\t0.0000\t\t-84433.5234\t\t-106761.3281\n",
            "Epoch 7:\t0.0000\t\t0.0000\t\t-125417.7031\t\t-154369.5469\n",
            "Epoch 8:\t0.0000\t\t0.0000\t\t-175679.8750\t\t-210164.0781\n",
            "Epoch 9:\t0.0000\t\t0.0000\t\t-234740.6406\t\t-274807.2812\n",
            "Epoch 10:\t0.0000\t\t0.0000\t\t-302984.0000\t\t-349773.3125\n",
            "\n",
            "(128, 64)-SGD\n",
            "Epoch 1:\t0.0195\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 2:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 3:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 4:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 5:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 6:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 7:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 8:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 9:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "Epoch 10:\t0.0391\t\t0.0714\t\tnan\t\tnan\n",
            "\n",
            "(128, 64)-RMSprop\n",
            "Epoch 1:\t0.0033\t\t0.0000\t\t-88.4806\t\t-217.1292\n",
            "Epoch 2:\t0.0000\t\t0.0000\t\t-338.5688\t\t-517.0267\n",
            "Epoch 3:\t0.0000\t\t0.0000\t\t-698.6729\t\t-954.6646\n",
            "Epoch 4:\t0.0000\t\t0.0000\t\t-1205.7826\t\t-1555.7573\n",
            "Epoch 5:\t0.0000\t\t0.0000\t\t-1891.4022\t\t-2360.9333\n",
            "Epoch 6:\t0.0000\t\t0.0000\t\t-2789.1440\t\t-3376.4639\n",
            "Epoch 7:\t0.0000\t\t0.0000\t\t-3901.2495\t\t-4624.1318\n",
            "Epoch 8:\t0.0000\t\t0.0000\t\t-5246.9531\t\t-6108.9351\n",
            "Epoch 9:\t0.0000\t\t0.0000\t\t-6879.1753\t\t-7947.5732\n",
            "Epoch 10:\t0.0000\t\t0.0000\t\t-8808.6348\t\t-10034.4707\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store average validation accuracy\n",
        "average_val_accuracy = {}\n",
        "\n",
        "# Iterate over architectures and optimizers\n",
        "for arch in architectures:\n",
        "    for opt_name, _ in optimizers:\n",
        "        key = f\"{arch}-{opt_name}\"\n",
        "        average_val_accuracy[key] = np.mean(results[key]['val_accuracy'])\n",
        "\n",
        "# Find the best parameters and approach\n",
        "best_params_approach = max(average_val_accuracy, key=average_val_accuracy.get)\n",
        "best_val_accuracy = average_val_accuracy[best_params_approach]\n",
        "\n",
        "print(f\"The best parameters and approach: {best_params_approach}\")\n",
        "print(f\"Average Validation Accuracy: {best_val_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOBRYFuAtvG5",
        "outputId": "728fd92a-192c-4e7f-eb6c-9c9e0219a33e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best parameters and approach: (64, 64)-SGD\n",
            "Average Validation Accuracy: 0.0714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('diabetes2.0.csv')\n",
        "\n",
        "# Split features and target\n",
        "X = data.drop('BloodPressure', axis=1)\n",
        "y = data['BloodPressure']\n",
        "\n",
        "# Splitting into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Function to create model\n",
        "def create_model(arch1=64, arch2=64, optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(arch1, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(arch2, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define a wrapper class for the Keras model\n",
        "class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, arch1=64, arch2=64, optimizer='adam', epochs=10, batch_size=32, verbose=0):\n",
        "        self.arch1 = arch1\n",
        "        self.arch2 = arch2\n",
        "        self.optimizer = optimizer\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.model = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = create_model(arch1=self.arch1, arch2=self.arch2, optimizer=self.optimizer)\n",
        "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=self.verbose)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self.model.predict(X) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Define grid search parameters\n",
        "architectures = {\n",
        "    'arch1': [64, 128],\n",
        "    'arch2': [64, 128],\n",
        "}\n",
        "\n",
        "optimizers = ['adam', 'sgd', 'rmsprop']\n",
        "\n",
        "param_grid = dict(arch1=architectures['arch1'],\n",
        "                  arch2=architectures['arch2'],\n",
        "                  optimizer=optimizers)\n",
        "\n",
        "# Perform grid search\n",
        "model = KerasClassifierWrapper()\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy', n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuWhlcScx_el",
        "outputId": "9eb12f02-b1af-4c79-eee6-87e1e76695d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: 0.039088 using {'arch1': 64, 'arch2': 64, 'optimizer': 'sgd'}\n",
            "0.000000 (0.000000) with: {'arch1': 64, 'arch2': 64, 'optimizer': 'adam'}\n",
            "0.039088 (0.000090) with: {'arch1': 64, 'arch2': 64, 'optimizer': 'sgd'}\n",
            "0.000000 (0.000000) with: {'arch1': 64, 'arch2': 64, 'optimizer': 'rmsprop'}\n",
            "0.000000 (0.000000) with: {'arch1': 64, 'arch2': 128, 'optimizer': 'adam'}\n",
            "0.039088 (0.000090) with: {'arch1': 64, 'arch2': 128, 'optimizer': 'sgd'}\n",
            "0.000000 (0.000000) with: {'arch1': 64, 'arch2': 128, 'optimizer': 'rmsprop'}\n",
            "0.000000 (0.000000) with: {'arch1': 128, 'arch2': 64, 'optimizer': 'adam'}\n",
            "0.039088 (0.000090) with: {'arch1': 128, 'arch2': 64, 'optimizer': 'sgd'}\n",
            "0.000000 (0.000000) with: {'arch1': 128, 'arch2': 64, 'optimizer': 'rmsprop'}\n",
            "0.000000 (0.000000) with: {'arch1': 128, 'arch2': 128, 'optimizer': 'adam'}\n",
            "0.039088 (0.000090) with: {'arch1': 128, 'arch2': 128, 'optimizer': 'sgd'}\n",
            "0.000000 (0.000000) with: {'arch1': 128, 'arch2': 128, 'optimizer': 'rmsprop'}\n"
          ]
        }
      ]
    }
  ]
}